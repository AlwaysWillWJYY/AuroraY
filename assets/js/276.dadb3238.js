(window.webpackJsonp=window.webpackJsonp||[]).push([[276],{735:function(a,r,t){"use strict";t.r(r);var e=t(1),s=Object(e.a)({},(function(){var a=this,r=a._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[r("h3",{attrs:{id:"_1-abstract"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-abstract"}},[a._v("#")]),a._v(" 1. Abstract")]),a._v(" "),r("ul",[r("li",[a._v("本文首次提出了利用代码的语义结构来学习代码表征的预训练模型GraphCodeBERT，基于Bert预训练模型实现，除了传统的MLM任务外，本文还提出了两个新的预训练任务（数据流边预测、源代码和数据流的变量对齐），基于数据流学习源代码的向量表征，在4个下游任务上取得了显著的提升效果。")])]),a._v(" "),r("h3",{attrs:{id:"_2-introduction"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-introduction"}},[a._v("#")]),a._v(" 2. Introduction")]),a._v(" "),r("p",[a._v("近年来，随着NLP领域的预训练模型不断发展，Bert、GPT等很多预训练模型被用到代码表征任务中来，比如code search, code summarization任务等等。然而，目前的方法是将代码片段视作一个token序列，忽略了代码的内在结构（提供了有用的代码语义信息）。")]),a._v(" "),r("p",[a._v("因此，本文提出了GraphCodeBert模型，基于数据流的方式做源代码的信息表征。数据流不同于AST，不会带来深层次不必要的复杂信息，数据流蕴含的是代码变量间“值从哪里来”的语义特征，使用该特征可以更有效的生成代码表征。")]),a._v(" "),r("p",[a._v("除了Bert中原有的Masked LM预训练模型外，本文还提出了两个预训练任务：")]),a._v(" "),r("ul",[r("li",[a._v("数据流边预测：通过预测数据流图的边，来学习代码的结构化表示")]),a._v(" "),r("li",[a._v("源代码和数据流的变量对齐：学习数据流图的节点来自源代码中的哪个token")])]),a._v(" "),r("p",[a._v("预训练阶段采用的数据集为：CodeSearchNet")]),a._v(" "),r("p",[a._v("本文将在4个下游任务上评估GraphCodeBert预训练模型，分别是：")]),a._v(" "),r("ul",[r("li",[a._v("代码搜索任务 code search")]),a._v(" "),r("li",[a._v("代码克隆检测任务 code clone detection")]),a._v(" "),r("li",[a._v("代码翻译任务 code translation")]),a._v(" "),r("li",[a._v("代码精炼任务 code refinement")])]),a._v(" "),r("p",[a._v("本文贡献：")]),a._v(" "),r("ul",[r("li",[a._v("GraphCodeBERT是第一个利用代码的语义结构来学习代码表征的预训练模型")]),a._v(" "),r("li",[a._v("提出了2个新的预训练模型，基于数据流学习源代码的向量表征")]),a._v(" "),r("li",[a._v("在4个下游任务上进行测试，发现测试结果得到显著提升")])]),a._v(" "),r("h3",{attrs:{id:"_3-data-flow-graph"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-data-flow-graph"}},[a._v("#")]),a._v(" 3. Data Flow Graph")]),a._v(" "),r("p",[a._v('数据流图是用来表示代码片段变量间的数据依赖关系的特征图，其中结点表示变量，结点之间的边表示的是变量之间值的流向，也就是“值从哪里来”。对于同一源代码，用不同的抽象语法结构可能会产生不同的表示，但其数据流是不变的，数据流图可以反映更深层次的信息。比如"v=max-min"，由于程序员并不总是按照规定命名变量，因此想要了解变量v的语义，可以考虑变量v的来源，来源于数据流中的max和min。数据流可以支持解析同一变量在不同的执行阶段所具有的不同的语义信息，比如下图中的x3,x7,x9,x11,虽然都是x，但语义信息却不同。')]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658735239796.png",alt:"1658735239796"}})]),a._v(" "),r("p",[a._v("那么如何构造数据流图呢，上图是构造数据流图的一个流程示例。对于一份源代码片段，首先使用标准的编译工具将其解析成AST，本文使用的解析工具是tree-sitter，可以支持本文涉及的6种编程语言（Ruby, Javascript, Go, Python, Java, Php）。")]),a._v(" "),r("p",[a._v('接下来，从生成的AST中提取出变量的序列，变量序列中的每一个元素都会成为数据流图中的一个结点。最后，基于变量序列和AST中提取出的变量之间的依赖关系，构建数据流图。所谓依赖关系指的是，对于"x=expr"，x依赖于右侧表达式中的所有变量，以此类推。数据流图是有向图，指向“值流向”的方向，a指向b代表b依赖于a。')]),a._v(" "),r("h3",{attrs:{id:"_4-model"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-model"}},[a._v("#")]),a._v(" 4.Model")]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658735662127.png",alt:"1658735662127"}})]),a._v(" "),r("p",[a._v("GraphCodeBert模型使用Bert做backbone，如上图所示，输入包含3部分：源代码C，注释W（该源代码片段的含义），和变量序列V，bert输入端为以上3个序列的连接X={[CLS] , W , [SEP] , C , [SEP] , V}。")]),a._v(" "),r("p",[a._v("和Bert的embedding方式类似，GraphCodeBert在对单词进行编码时采用了token embedding和position embedding结合的方式，position embedding也体现了变量在变量序列之中的位置信息，这个位置信息也对应着数据流中的不同结点。")]),a._v(" "),r("p",[a._v("GraphCodeBert模型使用了12个transformer encoder层来组成核心网络结构，采用12个attention head的多头注意力机制，包含Feed Forward层和Layer Normalization层等等，与我们熟知的transformer不同的是，本文中增加了一个Graph-Guided Masked Attention层，这个层与传统的Attention层的区别是在softmax计算权重之前需要增加一个参数M，功能是用来过滤无效的元素（在softmax之前加上负无穷）。")]),a._v(" "),r("h3",{attrs:{id:"_5-pre-train-task"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_5-pre-train-task"}},[a._v("#")]),a._v(" 5.Pre-train Task")]),a._v(" "),r("p",[a._v("GraphCodeBert模型的预训练任务有3个，首先是Bert传统的Masked LM模型，利用双向语言模型来学习语料库中单词的上下文信息。接下来是两个新加的任务：数据流边预测、源代码与数据流图的变量对齐，下面依次具体介绍。")]),a._v(" "),r("ul",[r("li",[a._v("数据流边预测 Edge Prediction")])]),a._v(" "),r("p",[a._v("该预训练任务的目的是让模型学习“值从哪里来”的信息，对应了模型架构图中的蓝色部分。预训练时随机采样20%的结点，通过mask矩阵（如果采样结点中有2个结点之间有边相连，就设为负无穷）来实现边的mask，然后让模型预测被mask的边。")]),a._v(" "),r("ul",[r("li",[a._v("源代码和数据流的变量对齐 Node Alignment")])]),a._v(" "),r("p",[a._v("该预训练任务的目的是让模型学习数据流图与源代码之间的对应关系，对应模型架构图中的橙色部分。与边预测不同的是，边预测任务学习的是变量序列V中两个结点之间的联系，而变量对齐任务学习的是源代码序列C和变量序列V之间的联系，也就是结点 $v_i$ 和单词 $c_j$之间的对应关系。")]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658738188512.png",alt:"1658738188512"}})]),a._v(" "),r("p",[a._v('如上图所示， x11与源代码中"return x"中的"x"对应。与边预测类似，该任务同样随机采样20%的结点，通过mask矩阵来实现结点与单词之间边的mask，模型通过预测被mask的边，学习结点和单词的关系。')]),a._v(" "),r("h3",{attrs:{id:"_6-实验"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_6-实验"}},[a._v("#")]),a._v(" 6. 实验")]),a._v(" "),r("p",[a._v("本文将在4个下游任务上评估GraphCodeBert预训练模型，并通过消融实验验证模型中每个部分的作用。")]),a._v(" "),r("ul",[r("li",[a._v("代码搜索任务 code search")])]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658738408378.png",alt:"1658738408378"}}),a._v(" 代码搜索任务的含义是给定一种自然语言输入，要求从一组候选代码中找到语义最相关的代码，使用的数据集是CodeSearchNet的数据集，使用代码文档的第一段作为query，以MRR(Mean Reciprocal Rank)作为评价指标，由上图可见，GraphCodeBert模型在每个语言的数据集上都表现优异。")]),a._v(" "),r("ul",[r("li",[a._v("代码克隆检测任务 code clone detection")])]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658738492138.png",alt:"1658738492138"}})]),a._v(" "),r("p",[a._v("代码克隆检测任务的含义是给定两个代码片段，要求度量其相似性，输出相似度，使用的数据集是BigCloneBench数据集，实验结果如上图所示，可见GraphCodeBert模型的准确率和召回率都很高。")]),a._v(" "),r("ul",[r("li",[a._v("代码翻译任务 code translation")])]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658738533224.png",alt:"1658738533224"}})]),a._v(" "),r("p",[a._v("代码翻译任务的含义是将一种编程语言范围为另一种编程语言，其主要用途是将遗留软件从平台的一种编程语言迁移到另一种编程语言，以Lucene、POI等开源项目为数据集，这些项目都有Java和C#的实现，任务中模型输入Java(C#)代码，输出与之对应的C#(Java)代码。实验结果如上图所示，GraphCodeBert模型的翻译准确率和BLEU得分都高于之前的模型。")]),a._v(" "),r("ul",[r("li",[a._v("代码精炼任务 code refinement")])]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658738806664.png",alt:"1658738806664"}})]),a._v(" "),r("p",[a._v("代码精炼任务也叫代码优化任务，旨在自动化的修复代码中的bug，使用Java数据集来实现，模型输入Java代码，输出修复bug之后的代码。实验结果如上图所示，GraphCodeBert模型优化的准确率和BLEU得分都高于之前的模型。")]),a._v(" "),r("ul",[r("li",[a._v("消融实验 ablation study")])]),a._v(" "),r("p",[r("img",{attrs:{src:"C:%5CUsers%5CAuroraY%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1658738744377.png",alt:"1658738744377"}})]),a._v(" "),r("p",[a._v("由上图消融实验可见，两个预训练任务Edge Prediction和Node Alignment都有效的提升了模型效果，当然，数据流的引进对模型的提升效果是最大的。")]),a._v(" "),r("h3",{attrs:{id:"_7-结论"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_7-结论"}},[a._v("#")]),a._v(" 7.结论")]),a._v(" "),r("p",[a._v("本文首次提出了利用代码的语义结构来学习代码表征的预训练模型GraphCodeBERT，基于Bert预训练模型实现，除了传统的MLM任务外，本文还提出了两个新的预训练任务（数据流边预测、源代码和数据流的变量对齐），基于数据流学习源代码的向量表征，在4个下游任务上取得了显著的提升效果 ,由此论文衍生出标书当中漏洞第二部分的一些想法。")])])}),[],!1,null,null,null);r.default=s.exports}}]);